{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "# Learning Features to Comapre Distributions\n",
    "\n",
    "$X,Y \\rightarrow P,Q$\n",
    "\n",
    "P: Dogs, Q: Fish\n",
    "\n",
    "How can we distinguish distributions? Perhaps difference? Perhaps ratio? These various means of differentiation lead to families of distributions.\n",
    "\n",
    "- Integral Probability metricks (earthmovers' difference)\n",
    "- F-digvergence $D_f(P,Q) = \\int_x q(x) {f({p(x)}\\over{q(x)}})$\n",
    "\n",
    "[8/28]\n",
    "\n",
    "If we learn a statistic to distinguish distributions, its not enough to know this statisitc, we need a mechansim to evaluate the confidence of the difference metric\n",
    "\n",
    "\n",
    "[10/28]\n",
    "\n",
    "Place kernel functions over various different observations. Use mean embedding of P and Q. Generate a 'witness function' to gcalculate the maximum mean discrepancy on a sample of observations.\n",
    "\n",
    "[13/28]\n",
    "\n",
    "For distributions of Dogs (P) and fish (Q), we can generate a matrix of MMDs to eval similarity (uncertain).\n",
    "\n",
    "[14/28]\n",
    "\n",
    "How do we choose a kernel to maximally distinguish P and Q. \n",
    "\n",
    "[15/28]\n",
    "\n",
    "Compare distirbutions of cases where P $=$ Q and P $\\neq$ Q\n",
    "\n",
    "[18/28]\n",
    "\n",
    "Maximize probability of P $\\neq$ Q, variance when P and Q are the same drops faster when P and Q are different. If you have enough samples, you can optimize test power by maximizing the $\\sqrt{MMD}$\n",
    "\n",
    "[20/28]\n",
    "\n",
    "[21/28]\n",
    "\n",
    "MMD is a quadradic time statistic, but can you use a linear time statistic? Perhaps we just look at the amplitude of the witness function - can we get a linear time statistic? Where is witness function best at discrimitating P and Q?\n",
    "\n",
    "[25/28]\n",
    "\n",
    "What about using the saure of the winess over teh variance of the sample space?\n",
    "\n",
    "[26/28]\n",
    "\n",
    "Generated face figure shows areas where the ability to distinguish between faces is best.\n",
    "\n",
    "[27/28]\n",
    "\n",
    "- Diversity of samples\n",
    " - MMD: distance between all samples\n",
    " - ME: similarities to reference features\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# $f-Gan$ Talk\n",
    "\n",
    "Learning Probabilistic Models\n",
    "\n",
    "- Propose parametric class of models, $\\mathcal{P}$ with parameters P, navigate the space of models and estimate a good one (called estimation). What's the difference between P and Q?\n",
    "- This process of determining P vs Q is called model estimation\n",
    "- Assumptions may be made to make gradients tractable (esp a tractible Likelihood function).\n",
    "\n",
    "Likelihood Free Model\n",
    "\n",
    "- Although a likelihood function exists, its computation is intractible\n",
    "- Excersize is then estimative\n",
    "- Need probability \n",
    "- Structured function classes \n",
    "- Loss function $\\rightarrow$ integrability metric\n",
    "\n",
    "Learning Prbabilistic Models\n",
    "- Integral metrics\n",
    " - P: Expectation\n",
    " - Q: Expectation\n",
    "- Scoring rules\n",
    " - P: Distribution\n",
    " - Q: Expectation\n",
    "- f-divergences\n",
    " - P: Distribution\n",
    " - Q: Distribution\n",
    " \n",
    "f-Divergences\n",
    "- Divergence between two distributions\n",
    " - f-genertor function, convex f(1) - 0\n",
    "- Every convex function f has a Fenchel conjugate f\\* so that ... (someting about supremum)\n",
    "\n",
    "Local convergence theorem\n",
    "\n",
    "Experiments\n",
    "\n",
    "Why fit images?\n",
    "- Proxy task: evolutionary tuned to assess whether image looks realistic\n",
    " - Perhaps by using our brains to asses models, we can transfer to other structures\n",
    "- Automatically discover structure in complex manifolds\n",
    "\n",
    "Future Directions\n",
    "- GANs are hard to train\n",
    "- If you consider f-divergence (integration over whole space), the process of training GAN, you might convolve distributions prior to feeding to discriminator.\n",
    "- Robost foolproof means of training GANs (6-12 months) (prediction)\n",
    "- With access to KL, you can get access to various optimization techniques. \n",
    "- GANs $\\rightarrow$ variational approximate inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Adversarilly Learned Inference ALI and BiGAN\n",
    "- arxiv:1606.00704, 1605.09782\n",
    "- Probabilistic Model\n",
    "\n",
    "\n",
    "## Deep directed generative models\n",
    "Goal: learn good directed generative models.\n",
    "- Extract latent variable 'z'\n",
    "- IN order to train a generative model you have to do some inference\n",
    "\n",
    "## GANs\n",
    "- $x ~ p(x|z)$ where x is data and z is latent variable.\n",
    "- GANs don't need an inference mechanism, but it would be awesome to infer the latent variables.\n",
    "\n",
    "## Adversarialy learned inference: Main Idea\n",
    "- Encoder distribution (inference)\n",
    "- Decoder distribution (generative model)\n",
    "\n",
    "## ALI: model diagram\n",
    "Generator learns conditionals \n",
    "\n",
    "- Encoder and Decoder try to fool the discriminator\n",
    "\n",
    "## ALI: Objective function\n",
    "- maximize the objective function wrt to parameter D\n",
    "\n",
    "## BIGAN: Encoder & Decoder are Inverses\n",
    "- x is sampled from data distribution\n",
    "- z is sampled from prior knowledge of latent variables\n",
    "\n",
    "## Toy Example\n",
    "\n",
    "## SVHN Dataset\n",
    "- z corresp to data x, moved to decoder, and generated a sample based on z (latent variable distribution)\n",
    "- It is surprising to see any similarity at all\n",
    "- Only connection between encoder and decoder is just the discriminator\n",
    "- Unsupervised training seems to preserve digit identity, but there is no reason to expect this from the models\n",
    "\n",
    "## CIFAR-10\n",
    "- Issue with GANs: underfitting maybe drops modes - CIFAR-10 is a good data et to study this\n",
    "\n",
    "## Celeb-A\n",
    "- Discriminator is good at using latent features with x such that correspondences between generated data and source data seem to be preserved.\n",
    "\n",
    "## Tiny ImageNet\n",
    "- Reconstructions show bad correspondances, but what\n",
    "\n",
    "## Interpolating in Latent Space\n",
    "- Infer latent variables, and traverse latent space \n",
    "- Similar to DCGAN paper\n",
    "- IN this case, the inference network allows us to go from one data point, interpolate to the next data point.\n",
    "\n",
    "## Conditional Generation\n",
    "- Encoder taking a label and feature, and calculate latent variable z\n",
    "- Decoder takes features and latent variable z and generates x\n",
    "- Discrinator to differentiate between encoder vs decoder model\n",
    "\n",
    "## Conditional Generation: CelebA\n",
    "\n",
    "## Semi-supervised experiments\n",
    "\n",
    "## Hierarchical ALI: model diagram\n",
    "- Perhaps there is a hierarchy of latent variables\n",
    "- One of the features in latent space can be convelutional in itself\n",
    "\n",
    "## Hierarchical ALI: CIFAR-10\n",
    "- Desire that we see reconstructions generate the same sort of 'content' without absolute duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Energy-Based GANs & Other Adversarial Things\n",
    "- Yann Le Cun\n",
    "\n",
    "## Unsupervised Learning\n",
    "- Capture dependencies between variables\n",
    "- Predict a subset of variables from any other subset\n",
    "- Learn a contrast function do determine if you are on of off the data manifold\n",
    "- How can you make sure energy outside of the manifold is higher to create a local minima near the data\n",
    "- There is uncertainty in output\n",
    " - Supervised - features/varaibles that are correlated (i.e. predicted from one another) are known ahead of time\n",
    " - Unsupervised - do not know which variables give predictive power to which other variables.\n",
    " \n",
    "## Seven Strategies to Shape the Energy Function\n",
    "1. constant volume of low energy surface for pca and k-means\n",
    "2. push down on energy of data points, push up everywhere else\n",
    " - create integral of expontential minus the energy - this is precisely what maximum likelihood does\n",
    " - Needs tractible partition function\n",
    " - If untractible, you can use surrogate to approximate partition function (WHAT ABOUT GREEN's FUNCTIONS!)\n",
    "3. Push down the energy of data points, push up on chosen locations\n",
    " - contrastive divergence, ratio matching, noise contrastive estimation, minimum probability flow\n",
    " - You don't run process for too long, keep at limited size. If you run a long time, you get Markov Chain monte carlo. \n",
    " - GANS are related to this point\n",
    "4. minimize gradient and maximize the curvature around data points\n",
    " - score matching\n",
    "6. Use a regularizer that limites the volume of space that has low-energy\n",
    " - sparse coding, sparse-auto-encoder\n",
    " - Echo point to sparse representation, pass to decoder. If decoder is linear, reconstruction is also linear, which corresponds to having a linear subspace for every code.\n",
    " - sparse autoencoder: start from input, go to code (make it sparse) which limits the number of points which can be generated, because number of configurations are limited.\n",
    " - Imposing sparsity decreases the volume of low-energy area\n",
    "7. if $E(Y) \\equiv ||Y-G(Y)||^2$ make $G(Y)$ as 'constant as possible'\n",
    "\n",
    "## Hard Part: Prediction Under Uncertainty\n",
    "- Want to train the Generator, but the Discriminator is an objective function. Any points which lie along surface of manifold is acceptable.\n",
    "\n",
    "## Energy-Based GAN\n",
    "- Novel aspect: discriminator is an auto-encoder.\n",
    "\n",
    "## EBGAN Loss function\n",
    "- Loss function employed for both Descriminator AND Generator\n",
    "\n",
    "## Style Transfer\n",
    "- Picture w/known category to different picture (style-wise)\n",
    "- Encoders $\\rightarrow$ Decoders $\\rightarrow$ Adversarial Networks\n",
    "\n",
    "Need to simplify the concept of a GAN to make accessible. Similarly to the 80s and 90s, neural networks didn't really catch on because there was really not too easy to train. With GANs, we need to figure out how to train them.\n",
    "\n",
    "GANs may be the best idea in the last 10-20 years, don't let this go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "# Conversational Agents\n",
    "\n",
    "Multimodal fusion.\n",
    "- Das et al. 2016: visual dialogue\n",
    "Common ground & convessation partners\n",
    "- How to introduce an idea of memory to conversations between humans and machines?\n",
    " - H: What do you do?\n",
    " - M: I'm a lawyer\n",
    " - H: What's your job?\n",
    " - M: I'm a doctor.\n",
    " \n",
    " Need to learn from linguists re: characteristics of conversation (perhaps as a feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# End to End learning of practical task-oriented dialog systems\n",
    "\n",
    "Interest in systems which give access to service API. \n",
    "\n",
    "## E2E Model Motivation and issues\n",
    "- Allow non ML experts to use tinkertoys to build models\n",
    "- Specifying state can be a problem in the real world\n",
    "- Fixing pipeline problems can be really hard - maybe a schema change affects things downstream\n",
    "- Still need to tackle\n",
    " - constraints from real world business systems\n",
    " - 'only allow transfer funds if reciept has been cleared': imagine if algo experimented with this for optimization!\n",
    "\n",
    "## Practical E2E LSTM\n",
    "- observe history of dialogue and output distribution over next set of possible actions\n",
    "- use word embeddings trained w/holdout set\n",
    "- Tackeling no-data: use an interactive learning approach\n",
    " - Expert can provide a few example dialogues, and deploy, collecting examples\n",
    " - system makes and corrects mistakes\n",
    " - w/reward signal, use reinforcement learning to learn on the fly\n",
    "\n",
    "Developer can correct dialogues and add actions associated with word embeddings. \n",
    "\n",
    "\n",
    "- User simulator: express distribution of behaviors over a population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Panel\n",
    "\n",
    "Q: What would you like to see in the next 5-10 years\n",
    "- A: Full human like dialogues\n",
    "- A: Move from dialogues into open ended dialogues with more modalities, less transactional. What about gesture, facial expression, etc\n",
    "- A: 5 years: nail the problem of building converational systems that interact in stuctured/semistructured ways. What about also knowledge graphs, etc, solve this. I.e.: Here's how to build ML dialogue system. 10 years: systems that learn what to do, and are sensitive to context of the world. \n",
    "- A: Twitter data set, ubuntu data set.\n",
    "\n",
    "Q: How should we collect and share data?\n",
    "- A: \n",
    "\n",
    "Q: Amazon alexa + competition to build social bot? Is it worth pursuing challenges, are we wasting time?\n",
    "- A: cool to stimulate community towards something new and different? Is it the right kind of challenge? Facebook is trying to design its own challenge.\n",
    "- A: main requirement of challenge is to make an entertaining system - but what about sophisticated knowledge?\n",
    "- A: Challenges are a great idea. \n",
    "- A: Amazon wants to see how others (students, etc) think about the problem. ASKAlexa (NLU + ASR) you can use this API if you're unfamiliar.\n",
    "\n",
    "Q: Loebner Prize: promise to pay a bunch of money if a machine can pass the turning test in a systematic chat interface. Entrants tend to be hand-engineered, but they don't really offer any kind of substance. They can't do anything for you, but they might seem human-like? We should distinguish between gaming the challenge vs building something really novel.\n",
    "- Need knowledge of past, as well as inference towards future states.\n",
    "\n",
    "Q: How can we deal with genericity in unsupervised dialogues?\n",
    "- A: There is some existing work to determining categories and solutions in that space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Q: What can lingueistics do that Neural Networks cannot?\n",
    "Q: Why should linguistics be isomorphic to scientific descriptions, rather than an approximation?\n",
    "- A: Maybe by furthering understanding into language we further ourself into "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Domain Adaption using Linguistic Knowledge\n",
    "\n",
    "## Supervised Natural Language Generation\n",
    "- Learn a mapping between sequenses of symbols and words\n",
    "- Long range dependancies (more context = better prediction)\n",
    "\n",
    "## Unsupervised NLG\n",
    "- Clustering in topics?\n",
    "- Mining structured dataset?\n",
    "\n",
    "## Minimising Human Input\n",
    "\n",
    "## Domain Adaptation in NLG\n",
    "- Generative approach using Deep Learning?\n",
    "- What about Dialogue Understanding?\n",
    " - How does input representation map to latent space?\n",
    " - Use input representation as a prior to map to other probabilities or domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Evaluating Goal Oriented E2E Dialog Systems\n",
    "- Human Evaluation\n",
    " - Pros: fluency test, task completion vs actual task\n",
    " - Cons: non reproducible\n",
    "## E2E candidate - Memory Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Awkward Silence? The Evolution of Social Dialogue Systems\n",
    "- Social chatbots are now more downloaded than social networking apps\n",
    "- The Alexa prize....\n",
    "\n",
    "## Extrinsic Motivation\n",
    "- User task success - perform task via interaction\n",
    "- Situated Task Success: use third party to achieve goal\n",
    "- Internal Goal Fulfilment: \n",
    "\n",
    "##\n",
    "\n",
    "##\n",
    "\n",
    "##\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
