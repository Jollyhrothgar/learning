{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning Features to Comapre Distributions\n",
    "\n",
    "$X,Y \\rightarrow P,Q$\n",
    "\n",
    "P: Dogs, Q: Fish\n",
    "\n",
    "How can we distinguish distributions? Perhaps difference? Perhaps ratio? These various means of differentiation lead to families of distributions.\n",
    "\n",
    "- Integral Probability metricks (earthmovers' difference)\n",
    "- F-digvergence $D_f(P,Q) = \\int_x q(x) {f({p(x)}\\over{q(x)}})$\n",
    "\n",
    "[8/28]\n",
    "\n",
    "If we learn a statistic to distinguish distributions, its not enough to know this statisitc, we need a mechansim to evaluate the confidence of the difference metric\n",
    "\n",
    "\n",
    "[10/28]\n",
    "\n",
    "Place kernel functions over various different observations. Use mean embedding of P and Q. Generate a 'witness function' to gcalculate the maximum mean discrepancy on a sample of observations.\n",
    "\n",
    "[13/28]\n",
    "\n",
    "For distributions of Dogs (P) and fish (Q), we can generate a matrix of MMDs to eval similarity (uncertain).\n",
    "\n",
    "[14/28]\n",
    "\n",
    "How do we choose a kernel to maximally distinguish P and Q. \n",
    "\n",
    "[15/28]\n",
    "\n",
    "Compare distirbutions of cases where P $=$ Q and P $\\neq$ Q\n",
    "\n",
    "[18/28]\n",
    "\n",
    "Maximize probability of P $\\neq$ Q, variance when P and Q are the same drops faster when P and Q are different. If you have enough samples, you can optimize test power by maximizing the $\\sqrt{MMD}$\n",
    "\n",
    "[20/28]\n",
    "\n",
    "[21/28]\n",
    "\n",
    "MMD is a quadradic time statistic, but can you use a linear time statistic? Perhaps we just look at the amplitude of the witness function - can we get a linear time statistic? Where is witness function best at discrimitating P and Q?\n",
    "\n",
    "[25/28]\n",
    "\n",
    "What about using the saure of the winess over teh variance of the sample space?\n",
    "\n",
    "[26/28]\n",
    "\n",
    "Generated face figure shows areas where the ability to distinguish between faces is best.\n",
    "\n",
    "[27/28]\n",
    "\n",
    "- Diversity of samples\n",
    " - MMD: distance between all samples\n",
    " - ME: similarities to reference features\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $f-Gan$ Talk\n",
    "\n",
    "Learning Probabilistic Models\n",
    "\n",
    "- Propose parametric class of models, $\\mathcal{P}$ with parameters P, navigate the space of models and estimate a good one (called estimation). What's the difference between P and Q?\n",
    "- This process of determining P vs Q is called model estimation\n",
    "- Assumptions may be made to make gradients tractable (esp a tractible Likelihood function).\n",
    "\n",
    "Likelihood Free Model\n",
    "\n",
    "- Although a likelihood function exists, its computation is intractible\n",
    "- Excersize is then estimative\n",
    "- Need probability \n",
    "- Structured function classes \n",
    "- Loss function $\\rightarrow$ integrability metric\n",
    "\n",
    "Learning Prbabilistic Models\n",
    "- Integral metrics\n",
    " - P: Expectation\n",
    " - Q: Expectation\n",
    "- Scoring rules\n",
    " - P: Distribution\n",
    " - Q: Expectation\n",
    "- f-divergences\n",
    " - P: Distribution\n",
    " - Q: Distribution\n",
    " \n",
    "f-Divergences\n",
    "- Divergence between two distributions\n",
    " - f-genertor function, convex f(1) - 0\n",
    "- Every convex function f has a Fenchel conjugate f\\* so that ... (someting about supremum)\n",
    "\n",
    "Local convergence theorem\n",
    "\n",
    "Experiments\n",
    "\n",
    "Why fit images?\n",
    "- Proxy task: evolutionary tuned to assess whether image looks realistic\n",
    " - Perhaps by using our brains to asses models, we can transfer to other structures\n",
    "- Automatically discover structure in complex manifolds\n",
    "\n",
    "Future Directions\n",
    "- GANs are hard to train\n",
    "- If you consider f-divergence (integration over whole space), the process of training GAN, you might convolve distributions prior to feeding to discriminator.\n",
    "- Robost foolproof means of training GANs (6-12 months) (prediction)\n",
    "- With access to KL, you can get access to various optimization techniques. \n",
    "- GANs $\\rightarrow$ variational approximate inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarilly Learned Inference ALI and BiGAN\n",
    "- arxiv:1606.00704, 1605.09782\n",
    "- Probabilistic Model\n",
    "\n",
    "\n",
    "## Deep directed generative models\n",
    "Goal: learn good directed generative models.\n",
    "- Extract latent variable 'z'\n",
    "- IN order to train a generative model you have to do some inference\n",
    "\n",
    "## GANs\n",
    "- $x ~ p(x|z)$ where x is data and z is latent variable.\n",
    "- GANs don't need an inference mechanism, but it would be awesome to infer the latent variables.\n",
    "\n",
    "## Adversarialy learned inference: Main Idea\n",
    "- Encoder distribution (inference)\n",
    "- Decoder distribution (generative model)\n",
    "\n",
    "## ALI: model diagram\n",
    "Generator learns conditionals \n",
    "\n",
    "- Encoder and Decoder try to fool the discriminator\n",
    "\n",
    "## ALI: Objective function\n",
    "- maximize the objective function wrt to parameter D\n",
    "\n",
    "## BIGAN: Encoder & Decoder are Inverses\n",
    "- x is sampled from data distribution\n",
    "- z is sampled from prior knowledge of latent variables\n",
    "\n",
    "## Toy Example\n",
    "\n",
    "## SVHN Dataset\n",
    "- z corresp to data x, moved to decoder, and generated a sample based on z (latent variable distribution)\n",
    "- It is surprising to see any similarity at all\n",
    "- Only connection between encoder and decoder is just the discriminator\n",
    "- Unsupervised training seems to preserve digit identity, but there is no reason to expect this from the models\n",
    "\n",
    "## CIFAR-10\n",
    "- Issue with GANs: underfitting maybe drops modes - CIFAR-10 is a good data et to study this\n",
    "\n",
    "## Celeb-A\n",
    "- Discriminator is good at using latent features with x such that correspondences between generated data and source data seem to be preserved.\n",
    "\n",
    "## Tiny ImageNet\n",
    "- Reconstructions show bad correspondances, but what\n",
    "\n",
    "## Interpolating in Latent Space\n",
    "- Infer latent variables, and traverse latent space \n",
    "- Similar to DCGAN paper\n",
    "- IN this case, the inference network allows us to go from one data point, interpolate to the next data point.\n",
    "\n",
    "## Conditional Generation\n",
    "- Encoder taking a label and feature, and calculate latent variable z\n",
    "- Decoder takes features and latent variable z and generates x\n",
    "- Discrinator to differentiate between encoder vs decoder model\n",
    "\n",
    "## Conditional Generation: CelebA\n",
    "\n",
    "## Semi-supervised experiments\n",
    "\n",
    "## Hierarchical ALI: model diagram\n",
    "- Perhaps there is a hierarchy of latent variables\n",
    "- One of the features in latent space can be convelutional in itself\n",
    "\n",
    "## Hierarchical ALI: CIFAR-10\n",
    "- Desire that we see reconstructions generate the same sort of 'content' without absolute duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Based GANs & Other Adversarial Things\n",
    "- Yann Le Cun\n",
    "\n",
    "## Unsupervised Learning\n",
    "- Capture dependencies between variables\n",
    "- Predict a subset of variables from any other subset\n",
    "- Learn a contrast function do determine if you are on of off the data manifold\n",
    "- How can you make sure energy outside of the manifold is higher to create a local minima near the data\n",
    "- There is uncertainty in output\n",
    " - Supervised - features/varaibles that are correlated (i.e. predicted from one another) are known ahead of time\n",
    " - Unsupervised - do not know which variables give predictive power to which other variables.\n",
    " \n",
    "## Seven Strategies to Shape the Energy Function\n",
    "1. constant volume of low energy surface for pca and k-means\n",
    "2. push down on energy of data points, push up everywhere else\n",
    " - create integral of expontential minus the energy - this is precisely what maximum likelihood does\n",
    " - Needs tractible partition function\n",
    " - If untractible, you can use surrogate to approximate partition function (WHAT ABOUT GREEN's FUNCTIONS!)\n",
    "3. Push down the energy of data points, push up on chosen locations\n",
    " - contrastive divergence, ratio matching, noise contrastive estimation, minimum probability flow\n",
    " - You don't run process for too long, keep at limited size. If you run a long time, you get Markov Chain monte carlo. \n",
    " - GANS are related to this point\n",
    "4. minimize gradient and maximize the curvature around data points\n",
    " - score matching\n",
    "6. Use a regularizer that limites the volume of space that has low-energy\n",
    " - sparse coding, sparse-auto-encoder\n",
    " - Echo point to sparse representation, pass to decoder. If decoder is linear, reconstruction is also linear, which corresponds to having a linear subspace for every code.\n",
    " - sparse autoencoder: start from input, go to code (make it sparse) which limits the number of points which can be generated, because number of configurations are limited.\n",
    " - Imposing sparsity decreases the volume of low-energy area\n",
    "7. if $E(Y) \\equiv ||Y-G(Y)||^2$ make $G(Y)$ as 'constant as possible'\n",
    "\n",
    "## Hard Part: Prediction Under Uncertainty\n",
    "- Want to train the Generator, but the Discriminator is an objective function. Any points which lie along surface of manifold is acceptable.\n",
    "\n",
    "## Energy-Based GAN\n",
    "- Novel aspect: discriminator is an auto-encoder.\n",
    "\n",
    "## EBGAN Loss function\n",
    "- Loss function employed for both Descriminator AND Generator\n",
    "\n",
    "## Style Transfer\n",
    "- Picture w/known category to different picture (style-wise)\n",
    "- Encoders $\\rightarrow$ Decoders $\\rightarrow$ Adversarial Networks\n",
    "\n",
    "Need to simplify the concept of a GAN to make accessible. Similarly to the 80s and 90s, neural networks didn't really catch on because there was really not too easy to train. With GANs, we need to figure out how to train them.\n",
    "\n",
    "GANs may be the best idea in the last 10-20 years, don't let this go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
